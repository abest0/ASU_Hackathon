{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASU Hackathon Mascot Detection notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train the model on the data set collected using ``classification_task/data_collection.ipynb`` notebook on the Jetbot\n",
    "\n",
    "We will use PyTorch Deep Learning framework to train model. We will use pre-trained ResNet-18 model as it is pretty much suited for our purposes. The pre-trained ResNet-18 provided by PyTorch is trained on ImageNet which has 1000 classes however the dataset we collected has 5 different classes (or labels), thus we will change last layer of the ResNet-18 in this notebook. \n",
    "\n",
    "We will be training on Jetson Nano for this notebook, however it is recommended to use Server grade GPUs for training purpose. Remember, number of GPU cores will have tremendous effect on execution time. The execution time of training neural network depends on following things:\n",
    "1. Neural Network Architecture (layers, depth etc.),\n",
    "2. Number of epoch, \n",
    "3. Number of training data,\n",
    "4. Batch Size\n",
    "\n",
    "More details on :\n",
    "1. [ResNet Image Recognition Architecture](https://arxiv.org/abs/1512.03385)\n",
    "2. [PyTorch Pre-Trained Networks](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "\n",
    "So lets get started on training model for classification task!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ``autotime`` library for timing each notebook cell. Just for convenience, such that you will kind of have idea which is the most time consuming cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.1.0 -f https://download.pytorch.org/whl/cu100/stable # CUDA 10.0 build\n",
    "!pip install torchvision==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/cpcloud/ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had discussed, we will be using PyTorch deep learning framework. To know best about this framwork, please have a look at [PyTorch talk from GTC On-Demand](https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s8817-pytorch%3a+a+fast+and+flexible+deep+learning+framework+%28presented+by+facebook%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload and extract dataset (if you are using Server Grade GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, you should upload the ``dataset.zip`` file that you created in the ``data_collection.ipynb`` notebook on the robot.\n",
    "\n",
    "You should then extract this dataset by calling the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://asu-hackathon.s3.amazonaws.com/dataset/asu_mascots_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q asu_mascots_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a folder named ``dataset`` appear in the file browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the ``ImageFolder`` dataset class available with the ``torchvision.datasets`` package.  We attach transforms from the ``torchvision.transforms`` package to prepare the data for training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 224\n",
    "image_height = 224\n",
    "dataset = datasets.ImageFolder(\n",
    "    'asu_dataset',\n",
    "    transforms.Compose([\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        transforms.Resize((image_width, image_height)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset into *training* and *test* sets.  The test set will be used to verify the accuracy of the model we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.2\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two ``DataLoader`` instances, which provide utilities for shuffling data, producing *batches* of images, and loading the samples in parallel with multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "\n",
    "Now, we define the neural network we'll be training.  The *torchvision* package provides a collection of pre-trained models that we can use.\n",
    "\n",
    "In a process called *[transfer learning](https://youtu.be/yofjFQddwHE)*, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
    "\n",
    "Important features that were learned in the original training of the pre-trained model are re-usable for the new task.  We'll use the ``resnet-18`` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``resnet-18`` model was originally trained for a dataset that had 1000 class labels, but our dataset only has six class labels!  We'll replace\n",
    "the final layer with a new, untrained layer that has only six outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(512,10) #original\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transfer our model for execution on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Jetson Nano, this might take little more time as compared to server grade GPUs, thus we will use ``applause`` sound clip to alert you that Jetson Nano has completed the training. \n",
    "For 100 each class i.e. 500 images it takes about 80 secs per epoch for transfer learning on ResNet-18. \n",
    "\n",
    "> Note: If you are using on server grade GPU, feel free to comment out following cell and last three lines from ``Train the Neural Network`` cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Load Dataset in to Batchs and Train the neural network\n",
    "\n",
    "In this, we will train model with different batch sizes with minimum batch size as 4 and maximum batch size as 16. For every batch size, we'll create two ``DataLoader`` instances, which provide utilities for shuffling data, producing *batches* of images, and loading the samples in parallel with multiple workers. For each batch, we will store best model based on test accuracy named ``best_model_<BATCH_SIZE>.pth``\n",
    "> We use different batch sizes to perform analysis of batch size vs training time and batch size vs accuracy. Once we are confident which is the best batch size based on analysis, we do not need to perform loops of different bacth sizes.\n",
    "\n",
    "Using the code below we will train the neural network for 30 epochs, saving the best performing model after each epoch.\n",
    "\n",
    "> An epoch is a full run through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        steps += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(test_loader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "\n",
    "torch.save(model, 'asu_best_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed training model for our own dataset with different batch sizes. You will see model files (.pth) for each batch size. \n",
    "So we performed:\n",
    "1. Split Dataset into 80% train data and 20% test data\n",
    "2. We created different batch sizes of data with minimum batch sizes as ``MIN_BATCH_SIZE`` and maximum batch sizes as ``MAX_BATCH_SIZE``\n",
    "3. We used Cross Entropy to calculate loss and Stochastic Gradient Descend as Optimizer\n",
    "4. We used ResNet-18 architecture, however we changed output layer size to accomodate our project requirements such that only 5 classes are being used. \n",
    "5. We saved best model with highest accuracy among all the epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is finished, you should see a file ``best_model.pth`` in the Jupyter Lab file browser.  \n",
    "\n",
    "> Note: Select ``Right click`` -> ``Download`` to download the model to your workstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
